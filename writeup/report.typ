#import "@preview/lovelace:0.3.0": *

#set page(
  paper: "a4",
  numbering: "1",
  columns: 2,
)

#place(
  top + center,
  float: true,
  scope: "parent",
  clearance: 3em,
)[
  // Add eth logo
  #align(center, image("figures/eth_logo.png", height: 5em))

  #align(
    center,
    text(20pt)[
      FNO based Foundational Models for Phase Field Dynamics
    ],
  )

  #align(
    center,
    text(14pt)[
      Benedict Armstrong \
      benedict.armstrong\@inf.ethz.ch
    ],
  )

  #align(
    center,
    text(14pt)[
      January 2025
    ],
  )

]

#set text(size: 11pt)
#set par(justify: true)
#show link: underline

= Introduction

This study implements a variant of Fourier Neural Operators (FNO) @li2021fourierneuraloperatorparametric to solve the one-dimensional Allen-Cahn equation. The Allen-Cahn equation is a fundamental partial differential equation (PDE) used to model phase separation processes in materials science. A primary challenge in this task is ensuring that the trained model can generalize to unseen initial conditions, a critical requirement for robust neural operator-based PDE solvers.

The FNO architecture is designed to learn mappings between function spaces, making it particularly well-suited for solving PDEs. Unlike traditional numerical solvers, FNOs operate in the frequency domain, enabling efficient learning of complex spatial and temporal dependencies and invariance to the mesh. The implementation of the model is structured across two primary files: lib/model.py, which defines the neural network architecture, and lib/layers.py, which contains the necessary layer definitions.

This work evaluates the performance of the trained model in predicting the solution of the Allen-Cahn equation for various initial conditions, including those not seen during training. The results highlight the model's ability to generalize and provide insight into the applicability of neural operators for PDE-driven problems.

#let body = [
  = Data Generation

  For this task, we were asked to generate our own data for training and testing. The data was generated by solving the _Allen-Cahn_ equation using the `scipy.solve_ivp` solver and the `RK45` method implemented in `lib/allen_cahn.py`. Due to the exponential dependence of the convergence behavior on the $epsilon$ parameter the model was train on a range of $epsilon$ values. I used the suggested values of $epsilon = 0.1,
0.05, 0.02$ and $N_"train" = 400$ total samples. The `data.ipynb` notebook contains an interactive visualization of the generated data for different $epsilon$ values and initial conditions. Solutions for lower epsilon values converge very quickly and sampling the same $5$ equidistant timesteps for all $epsilon$ values doesn't capture the trajectories well. To mitigate this I first scaled the time dimension by $epsilon$ and then (only for training data) sampled $5$ random timesteps from a non uniform distribution in $[0, 1]$. The distribution was generated using the following code snippet and is shown in @eval_points.
  ```py
  t_train = np.logspace(0, 1, 30, base=3)
  t_train = (t - 1) / t[-1]
  ```

  #figure(
    image("../figures/eval_points.png", width: 125%),
    caption: [Evaluation points for #text(fill: blue, [#sym.circle.filled]) training /#text(fill: orange, size:10pt, [#sym.crossmark.heavy]) testing],
    gap: 15pt,
  ) <eval_points>

  This ensures we use only the 5 temporal snapshots for training and testing and that the snapshots capture a greater range of the solution dynamics.


  = Model

  As mentioned above I used a very similar model to the one used for the _TFNO_ task only being adapted to accept tensors with last dimension 2 and include the $epsilon$ in the time embedding.

  = Training
  // Analysis of your model’s performance, including error metrics and convergence behavior

  The data was split into a training and test set with a ratio of $80:20$ and trained for $100$ epochs. The model was trained using the `Adam` optimizer and the `CosineAnnealingWarmRestarts` scheduler. The full parameters used to train the model are listed in @ac_params. The model achieved an average relative $L_2$ error of $0.019$ on the entire test set, @ac_l2_errors shows the errors and some examples are plotted in @ac_test_examples We can also see in @ac_l2_errors that the model performs similarly well on all tested types of initial conditions.

  #figure(
    table(
      columns: 2,
      stroke: (x: none),
      row-gutter: (2.5pt, auto),
      table.header[Dataset (IC Type), $Delta t$][rel. $L_2$ err.],
      [Test (All), $Delta t = [0.25, 0.5, 0.75, 1.]$], [$0.019$],
      [Test (All), $Delta t = 1.00$], [$0.020$],
      [Test (Fourier), $Delta t = 1.00$], [$0.024$],
      [Test (GMM), $Delta t = 1.00$], [$0.015$],
      [Test (Piecewise), $Delta t = 1.00$], [$0.021$],
    ),
    caption: [Predictions on test dataset at different $Delta t$],
  ) <ac_l2_errors>

  = Results
  // Visualization of the generated data across different regimes
  // Discussion of generalization properties, supported by plots comparing predictions to ground truth
  // Investigation of how the model handles challenging cases, such as very small ϵ values or high-frequency initial conditions
  // Think carefully about your choices of ϵ and the expected behavior of solutions as ϵ → 0 – how do the dynamics change, in terms of the competing diffusive versus nonlinear effects?
  //
  After training, the model was evaluated on a set of out of distribution (OOD) data. The model achieved a relative $L_2$ error of $0.028$. Examples of the model's predictions on the OOD data can be found in the `eval_ood.ipynb` notebook and in @ac_ood_examples The model performs moderately well on the OOD data as shown in @ac_ood_errors.

  #figure(
    table(
      columns: 2,
      stroke: (x: none),
      row-gutter: (2.5pt, auto),
      table.header[Dataset, $Delta t$][Average rel. $L_2$ err.],
      [OOD, All $Delta t$], [$0.0287$],
      [OOD, $Delta t = 1.00$], [$0.2189$],
    ),
    caption: [Predictions on OOD dataset at different $Delta t$],
  ) <ac_ood_errors>


  == Generalization to unseen $epsilon$ values & initial conditions

  #figure(
    image("../figures/ac_ood_l2_vs_eps.png"),
    caption: [$L_2$ error on OOD data vs. $epsilon$ for the Allen-Cahn model],
  ) <ac_ood_l2_vs_eps>

  @ac_ood_l2_vs_eps shows well how the model fails to extrapolate to high unseen epsilons while performing reasonably well on interpolated and lower epsilon values. The orange line shows the error when only the initial conditions are different (higher frequency and sharper transitions) which yields good results. @ac_ood_examples shows some of the model's predictions on OOD data.


  == High Frequency Initial Conditions and low $epsilon$

  #figure(
    image("../figures/ood_low_ep.png"),
    caption: [$L_2$ error on OOD (low $epsilon$) data vs. $epsilon$ for the Allen-Cahn model],
  ) <ac_ood_low_eps_l2_vs_eps>

  The model was also tested on super low $epsilon$ values such as $epsilon = 0.0001, 0.001, 0.005$ showing a significant decrease in performance. @ac_ood_hf_examples shows some examples of high frequency initial conditions, which seem The model's predictions on the test data for different $epsilon$ values can be found in the `eval_ood_hf_low_e.ipynb` notebook.

  = Conclusion & Improvements

  The model performs well on the test data and reasonably well on OOD data for the seen, interpolated and lower $epsilon$ values. The model fails to generalize to larger extrapolated $epsilon$ values. I also tried fine-tuning the model on the OOD data but only got moderate or no improvements in the relative $L_2$ error.

  = Proof for the stability of the _Allen-Cahn_ equation

  A More detailed results including accounting for discretization error and further discussion can be found in @bartels2015numerical[Chapter 6].

  _Proof_:

  With $c_f = "sup"_(s in B_1(0))|f'(s)|$, we have

  $
    |f(x) - f(y)| = <= c_f |x - y|
  $

  for all $x, y in RR$. The difference $delta = u - tilde(u)$ satisfies

  $
    (partial_t, v) + (gradient delta, gradient v) = - epsilon^2(f(u) - f(tilde(u)), v)
  $

  for almost every $t in (0, T)$ and all $v in H_0^1(Omega)$. We use the test function $v = delta$ and get

  $
    1 / 2 d / (d t) ||delta||^2 + ||gradient delta||^2 &<= c_f epsilon^(-2) ||delta||^2 \
    &<= c_f epsilon^(-2) ||delta||^2 + 1 / 2 (||delta ||^2 + ||gradient delta||^2) \
    &<= 1 / 2 (1 + 2 c_f epsilon^(-2)) ||delta||^2 + 1 / 2 ||gradient delta||^2
  $

  Which absorbs the $||gradient delta||^2$ term. Integrating over $(0, T')$ we get

  $
    ||delta(T')||^2 + integral_0^T'||gradient delta||^2 dif t <=& ||delta(0)||^2 + \ & (1 + 2c_f epsilon^(-2) )integral_0^T' ||delta||^2 dif t
  $

  We define $A = ||delta(0)||^2$ , $b = (1 + 2c_f epsilon^(-2))$ and

  $
    y(t) = ||delta(t)||^2 + integral_0^t ||gradient delta||^2 dif s
  $

  and apply the Gronwall lemma @bartels2015numerical[Proposition 6.2] for nonnegative functions $y in C([0, T])$

  $
    y(T') <= A + integral_0^T' b(t) y(t) dif t
  $

  with a nonnegative function $b in L^1(0, T')$. We have

  $
    y(T') <= A exp(integral_0^T b(t) dif t)
  $

  which proves the claim:

  $
    ||delta(t)||^2 + integral_0^t ||gradient delta||^2 dif s<= ||delta(0)||^2 exp(T + T 2c_f epsilon^(-2))
  $

  #align(
    right,
    text(14pt)[
      $qed$
    ],
  )

  Even for moderately large $epsilon$ values such as $epsilon approx 10^(-1)$ the exponential factor which depends on $epsilon^(-2)$ makes this error estimate only moderately useful.

]

#body

#bibliography("refs.bib")

#pagebreak()

// Appendix
#set page(
  numbering: "A",
  columns: 1,
)
#counter(page).update(1)
#counter(heading).update(0)
#set heading(numbering: none)

= Appendix

#let appendix = [

  #figure(
    table(
      columns: 2,
      stroke: (x: none),
      row-gutter: (2.5pt, auto),
      table.header[*Parameter*][*Value*],
      [Epochs], [100],
      [Batch size], [1024],
      [Optimizer], [Adam],
      [weight_decay], [1e-5],
      [Learning rate], [0.0005],
      [Scheduler], [CosineAnnealingWarmRestarts],
      [eta_min], [1e-6],
      [T_0], [20],
      [Loss], [Relative $L_2$ error],
      [Modes], [16],
      [Width], [64],
      [Fourier Layers], [4],
    ),
    caption: [Parameters used for training Allen-Cahn FNO model],
  ) <ac_params>

  #figure(
    image("../figures/ac_test_examples.png"),
    caption: "Predictions on test data for the Allen Cahn TFNO model",
  ) <ac_test_examples>

  #figure(
    image("../figures/ac_ood_examples.png"),
    caption: "Predictions on OOD data for the Allen Cahn TFNO model",
  ) <ac_ood_examples>

  #figure(
    image("../figures/ac_ood_hf_examples.png"),
    caption: "Predictions on OOD (High Frequency IC) data for the Allen Cahn TFNO model",
  ) <ac_ood_hf_examples>


  #figure(
    image("../figures/ac_ood_low_eps_examples.png"),
    caption: [Predictions on OOD (Low $epsilon$) data for the Allen Cahn TFNO model],
  ) <ac_ood_low_e_examples>


]

#appendix
